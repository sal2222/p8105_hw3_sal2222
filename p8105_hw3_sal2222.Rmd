---
title: "p8105_hw3_sal2222"
author: "Stephen Lewandowski"
date: "October 15, 2018"
output: 
  github_document:
    toc: true
---


```{r setup, include = FALSE}

library(tidyverse)
library(readxl)
library(devtools)
library(p8105.datasets) #devtools::install_github("p8105/p8105.datasets")

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1: BRFSS


I will load and format the Behavioral Risk Factors Surveillance System (BRFSS) for Selected Metropolitan Area Risk Trends (SMART) data from the p8105.datasets package.

```{r load and clean BRFSS}

data("brfss_smart2010")

brfss <- brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-c(class, topic, question, sample_size, confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  separate(county, into = c("remove", "county"), sep = "- ") %>%
  select(year, state, county, poor, fair, good, very_good, excellent)
  
brfss
   
```


I will answer questions using this BRFSS dataset.

*In 2002, which states were observed at 7 locations?*

```{r brfss states with 7 locations}

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarise(number = n()) %>% 
  filter(number == 7)

```

In 2002, 7 locations were observed in **Connecticut, Florida, and North Carolina**.    



*Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.*

```{r spaghetti plot}
brfss %>% 
  group_by(state, year) %>% 
  summarise(number = n()) %>% 
  ggplot(aes(x = year, y = number)) +
    geom_line(aes(color = state, alpha = 0.5)) +
    geom_smooth(method = "loess", se = TRUE) + 
    labs(
      title = "Locations observed by state, 2002-2010",
      x = "Year",
      y = "Number of observations",
      caption = "Data from BRFSS SMART 2010"
    ) +
    viridis::scale_color_viridis(
      name = "State", 
    discrete = TRUE
    ) +
    theme(legend.position = "none")
```



This plot is useful for showing the general trend of locations by state from 2002 to 2010. However, it is very difficult to identify individual states, and a legend for all of the states occupies too much space, if included. A loess smoothed conditional mean curve shows a very gradual increase in number of locations observed by state over this time period.  



*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.*

```{r brfss table NY state, warning = FALSE}

brfss %>% 
  filter(year == 2002 | 2006 | 2010) %>% 
  filter(state == "NY") %>% 
  group_by(year) %>%
  summarise(mean_excellent = mean(excellent),
            sd_excellent = sd(excellent)) %>% 
  knitr::kable(digits = 1, caption = "Proportion of “Excellent” responses across locations in NY State")
  
```


The table above shows the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State. Nothing in particular stands out from the results. The highest proportion of "Excellent" responses occurred in 2002 (24.0%) and the lowest occurred in 2007 (21.1%).   


*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time*

```{r BFRSS five panel plot, warning = FALSE}

brfss %>% 
  select(-county) %>% 
  group_by(year, state) %>% 
  summarise_all(funs(mean = "mean")) %>% 
  gather(key = rating, value = mean_proportion, poor_mean:excellent_mean) %>%
  mutate(rating = 
      factor(rating, c("poor_mean", "fair_mean", "good_mean", "very_good_mean", "excellent_mean"))) %>% 
  ggplot(aes(x = year, y = mean_proportion)) +
    geom_line(aes(color = state, alpha = 0.5)) +
    geom_smooth(method = "loess", se = TRUE) + 
    facet_grid(~rating) + 
    labs(
      title = "Locations observed by state, 2002-2010",
      x = "Year",
      y = "Number of observations",
      caption = "Data from BRFSS SMART 2010"
    ) +
    viridis::scale_color_viridis(
      name = "State", 
    discrete = TRUE
    ) +
    theme(legend.position = "none", 
          axis.text.x = element_text(size = 5))

```



The five-panel plot above displays the average proportion in each response category in the BRFSS "Overall Health" topic, averaged across county-level regions in each state, with a loess smoothed conditional mean curve and 95% confidence level. We see the distribution of state-level averages over time from 2002 to 2008. "Very good" makes up the highest proportion of responses, followed by "good", "excellent", "fair", and lastly "poor". For the most part, the trend of responses remained consistent over time. The proportion of "very good" responses increases slightly, and "excellent" responses decrease slightly over this period.    




#Problem 2: Instacart

First, I will load the Instacart data.
```{r load Instacart data}
data("instacart")

instacart
```


This Instacart Online Grocery Shopping 2017 dataset comes from an anonymized dataset with over 3 million online grocery orders from more than 200,000 Instacart users. This version of the data contains `r nrow(instacart)` observations of products in the orders. There are `r ncol(instacart)` variables in this dataset that describe order, product, and customer identifiers, the order in which the item was added to a user's cart, whether the product had been previously ordered by the user, the order sequence from the user, the hour and day of the week the order was placed, day since prior order, as well as product name, aisle, and department information. Our data contains `r instacart %>% summarize(n_distinct = n_distinct(order_id))` unique orders for `r instacart %>% summarize(n_distinct = n_distinct(product_id))` unique products. The average number of items in an order was `r instacart %>% group_by(order_id) %>% summarize(n = n()) %>% summarize(mean = mean(n))`.   


*How many aisles are there, and which aisles are the most items ordered from?*

```{r instacart aisles}
instacart %>% 
  summarize(n_distinct = n_distinct(aisle_id))

instacart %>%
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))

```

There are **134** aisles in the database. The most items are ordered from **Fresh Vegetables (150,609), Fresh Fruits (150,773), and Package Vegetables and Fruits (78,493)**. 
Users seem to order healthy foods from this service.


*Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.*


```{r plot items in aisles ordered by aisle ID}

instacart %>%
 group_by(aisle_id) %>% 
  summarize(n = n()) %>%
  ggplot(aes(x = aisle_id, y = n)) +
    geom_bar(stat = "identity") +
        labs(
     title = "Number of items ordered in each Instacart aisle",
      x = "Aisle ID",
      y = "Number of items ordered",
      caption = "Data from Instacart"
    )  +
    scale_x_continuous(breaks = c(0, 20, 40, 60, 80, 100, 120, 140)) +
    scale_y_continuous(labels = scales::comma)

```

The plot above shows the number of items ordered in each aisle with the x-axis arranged sequentially by the aisle identification number. This display may be useful to get an idea of layout efficiency. We see that popular aisles are fairly evenly dispersed throughout the span of aisles. 


```{r plot items in aisles ordered by number of items}

instacart %>%
 group_by(aisle_id) %>% 
  summarize(n = n()) %>%
  ggplot(aes(x = reorder(aisle_id, -n), y = n)) +
    geom_bar(stat = "identity") +
    labs(
     title = "Number of items ordered in each Instacart aisle",
      x = "Aisle ID",
      y = "Number of items ordered",
      caption = "Data from Instacart"
    ) +
    theme(axis.text.x = element_text(angle = 90, size = 4)) +
     scale_y_continuous(labels = scales::comma)
```

The plot above shows the number of items ordered in each aisle with the x-axis arranged from highest to lowest. This display is useful to visualize differences between the items ordered per aisle. However, because the 134 aisles are out of order, it is not easy to locate a specific aisle of interest. 


*Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”*

```{r popular item table}
instacart %>%
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  summarize(most_popular_item = first(product_name)) %>% 
  knitr::kable()
```

The table above shows the most popular item in three selected aisles. The items (corn starch, dog biscuits, salad) make sense for their respective aisles.


*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*

```{r time of order table, warning = FALSE}
instacart %>%
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  mutate(order_dow = recode_factor(order_dow,
       "0" = "Sunday",
       "1" = "Monday",
       "2" = "Tuesday",
       "3" = "Wednesday",
       "4" = "Thursday",
       "5" = "Friday",
       "6" = "Saturday")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_day) %>% 
  knitr::kable(digits = 2)

```

The table above displays the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream were ordered on each day of the week. The mean hour values are based on a 24 hour clock, and represent fraction of an hour, rather than hours and minutes. From the `order_dow` variable, day `0` was assumed to represent Sunday. We see that every day except for Friday, the ice cream is ordered later in the day than the apples. Lunchtime and late afternoon are the most popular order times, except for the Friday morning coffee ice cream.     


#Problem 3: NY NOAA

I will load the NY NOAA data from the p8105.datasets package.

```{r load NOAA data}
data("ny_noaa")
ny_noaa
```

This dataset contains measurements from all New York state weather stations in the Global Historical Climatology Network (GHCN) from January 1, 1981 through December 31, 2010. It includes variables for  maximum and minimum daily temperatures, along with daily precipitation, snowfall, and snow depth. The five measurement variables are grouped by station identification number and date, for a total of 7 variables. There are a total of `r nrow(ny_noaa)` daily observations in this dataset. The units of snowfall and snow depth are in millimeters. Precipitation is in tenths of millimeters, and temperatures are in tenths of degrees Celsius.

Missing data, depending on the variable of interest, can be extensive. Approximately half of the stations only report precipitation. There are `r ny_noaa %>%  complete.cases() %>% sum()`  complete observations with all five measurements. There are `r is.na(ny_noaa$prcp) %>% sum()` missing precipitation measurements, `r is.na(ny_noaa$snow) %>% sum()` missing snowfall measurements, `r is.na(ny_noaa$snwd) %>% sum()` missing snow depth measurements, `r is.na(ny_noaa$tmax) %>% sum()` maximum temperature measurements, and `r is.na(ny_noaa$tmin) %>% sum()` minimum temperature measurements.     


I will now clean the data and create separate variables for year, month, and day. I will also convert units of precipitation to millimeters and temperatures to degrees Celsius. To execute the temperature unit conversions, it was necessary to change the class from character to numeric. 

```{r clean noaa data}
ny_noaa <- ny_noaa %>%
  separate(., "date", c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.numeric(tmax),
            tmin = as.numeric(tmin),
            prcp = prcp / 10, 
            tmax = tmax / 10,
            tmin = tmin / 10)

ny_noaa
```

*For snowfall, what are the most commonly observed values? Why?*  

```{r snowfall}
ny_noaa %>% 
   group_by(snow) %>% 
   summarize(n = n()) %>% 
   top_n(10) %>% 
   arrange(desc(n))
  
```

The most commonly observed value is **0 mm**, with over 2 million observations. This makes sense in New York State due to it seasonal weather. Also, even if there is light snow, the snowfall may not be measurable. Following observations where snowfall was not reported, the next most commonly observed values were **25 mm, 13 mm, and 51 mm**.  As the snowfall was likely measured in inches, these common values correspond to 1 inch, half inch, and 2 inch reports. 


*Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?* 

```{r ny noaa 2 panel plot, warning = FALSE}

ny_noaa %>% 
  filter(month == c("01", "07")) %>%
  select(-(c("day", "prcp", "snow", "snwd", "tmin"))) %>%
  mutate(year = as.numeric(year)) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  mutate(month = recode_factor(month,
            "01" = "January",
            "07" = "July")) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
    geom_line(aes(color = id), alpha = 0.5, size = 0.05) +
    facet_grid(~month) + 
    labs(
      title = "New York average maximum temperatures",
      x = "Year",
      y = "Mean maximum daily temperature (deg C)",
      caption = "Data from the rnoaa package"
    ) + 
    scale_x_continuous(breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
    theme(legend.position = "none")
  
```

There is not an apparant long-term trend in the January or July mean maximum temperatures over the observed thirty years. Oscillations are observed over two to four year periods. There is more year-to-year and within-year variability in the January data compared to July. Outliers are more apparant in the July panel, and exhibit lower than expected temperatures. The largest outlier is seen in July 1988. Other outlier points occur in July 1984 and July 2004.


*Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option);and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.*

```{r noaa two panel full data, warning = FALSE}

ny_noaa %>% 
  select(-(c("prcp", "snow", "snwd"))) %>% 
    ggplot(aes(x = tmin, y = tmax)) +
    geom_hex(bins = 50) +
    geom_smooth() +
    labs(
      title = "Hexagonal heatmap of New York maximum vs. minimum daily temperatures, 1981-2010",
      x = "Minimum daily temperature (deg C)",
      y = "Maximum daily temperature (deg C)",
      caption = "Data from the rnoaa package") +
    viridis::scale_fill_viridis()


ny_noaa %>% 
  select(-(c("prcp", "snwd", "tmax", "tmin"))) %>% 
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    labs(
      title = "Violin plot of New York distribution of snowfall, 1981-2010",
      x = "Year",
      y = "Snowfall (mm)",
      caption = "Data from the rnoaa package") +
    theme(axis.text.x = element_text(angle = 90, size = 10)) 

```

In the first panel, we see maximum daily temperatures plotted against minimum daily temperatures for the entire New York dataset in a hexagonal heatmap with a GAM smoothed conditional mean curve. The aggregated relationshp appears mostly linear in the middle where most of the observations occur, but shows a soft "S-shape" at the extremes. Two hotspots appear in the central band at maximum temperatures of approximately 5 degrees C and 27 degrees C (corresponding to 0 and 16 degreee C minimum temperatures.)  

In the second panel, we see a violin plot with quartile lines of the distribution of snowfall values greater than 0 and less than 100 from 1981 to 2010. The snowfall distribution only appears to vary slightly year-to-year, with no clear long-term trends from this selection. In each year, we observe four nodes of common observations, corresponding to half-inch, one inch, two inch, and three inch recordings. 



