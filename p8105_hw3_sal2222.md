p8105\_hw3\_sal2222
================
Stephen Lewandowski
October 15, 2018

-   [Problem 1: BRFSS](#problem-1-brfss)
-   [Problem 2: Instacart](#problem-2-instacart)
-   [Problem 3: NY NOAA](#problem-3-ny-noaa)

Problem 1: BRFSS
================

I will load and format the Behavioral Risk Factors Surveillance System (BRFSS) for Selected Metropolitan Area Risk Trends (SMART) data from the p8105.datasets package.

``` r
data("brfss_smart2010")

brfss <- brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-c(class, topic, question, sample_size, confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  separate(county, into = c("remove", "county"), sep = "- ") %>%
  select(year, state, county, poor, fair, good, very_good, excellent)
  
brfss
```

    ## # A tibble: 2,125 x 8
    ##     year state county                  poor  fair  good very_good excellent
    ##    <int> <chr> <chr>                  <dbl> <dbl> <dbl>     <dbl>     <dbl>
    ##  1  2002 AK    Anchorage Municipality   5.9   8.6  23.8      33.7      27.9
    ##  2  2002 AL    Jefferson County         5.9  12.1  32.7      30.9      18.5
    ##  3  2002 AR    Pulaski County           4.2  12.5  29.9      29.3      24.1
    ##  4  2002 AZ    Maricopa County          4.6  10.3  26.9      36.6      21.6
    ##  5  2002 AZ    Pima County              3.9   7.5  31.9      30.1      26.6
    ##  6  2002 CA    Los Angeles County       4.5  14.3  28.7      29.8      22.7
    ##  7  2002 CO    Adams County             4.2  14.4  29        31.2      21.2
    ##  8  2002 CO    Arapahoe County          2.1   8    29.3      35.2      25.5
    ##  9  2002 CO    Denver County            3    11.1  36.6      27.1      22.2
    ## 10  2002 CO    Jefferson County         2.4  11.4  26.3      36.6      23.4
    ## # ... with 2,115 more rows

I will answer questions using this BRFSS dataset.

*In 2002, which states were observed at 7 locations?*

``` r
brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarise(number = n()) %>% 
  filter(number == 7)
```

    ## # A tibble: 3 x 2
    ##   state number
    ##   <chr>  <int>
    ## 1 CT         7
    ## 2 FL         7
    ## 3 NC         7

In 2002, 7 locations were observed in **Connecticut, Florida, and North Carolina**.

*Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.*

``` r
brfss %>% 
  group_by(state, year) %>% 
  summarise(number = n()) %>% 
  ggplot(aes(x = year, y = number)) +
    geom_line(aes(color = state, alpha = 0.5)) +
    geom_smooth(method = "loess", se = TRUE) + 
    labs(
      title = "Locations observed by state, 2002-2010",
      x = "Year",
      y = "Number of observations",
      caption = "Data from BRFSS SMART 2010"
    ) +
    viridis::scale_color_viridis(
      name = "State", 
    discrete = TRUE
    ) +
    theme(legend.position = "none")
```

<img src="p8105_hw3_sal2222_files/figure-markdown_github/spaghetti plot-1.png" width="90%" />

This plot is useful for showing the general trend of locations by state from 2002 to 2010. However, it is very difficult to identify individual states, and a legend for all of the states occupies too much space, if included. A loess smoothed conditional mean curve shows a very gradual increase in number of locations observed by state over this time period.

*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.*

``` r
brfss %>% 
  filter(year == 2002 | 2006 | 2010) %>% 
  filter(state == "NY") %>% 
  group_by(year) %>%
  summarise(mean_excellent = mean(excellent),
            sd_excellent = sd(excellent)) %>% 
  knitr::kable(digits = 1, caption = "Proportion of “Excellent” responses across locations in NY State")
```

|  year|  mean\_excellent|  sd\_excellent|
|-----:|----------------:|--------------:|
|  2002|             24.0|            4.5|
|  2003|             21.9|            4.6|
|  2004|             21.2|            5.1|
|  2005|             21.7|            5.1|
|  2006|             22.5|            4.0|
|  2007|             21.1|            5.1|
|  2008|             22.8|            4.5|
|  2009|             23.3|            4.3|
|  2010|             22.7|            3.6|

The table above shows the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State. Nothing in particular stands out from the results. The highest proportion of "Excellent" responses occurred in 2002 (24.0%) and the lowest occurred in 2007 (21.1%).

*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time*

``` r
brfss %>% 
  select(-county) %>% 
  group_by(year, state) %>% 
  summarise_all(funs(mean = "mean")) %>% 
  gather(key = rating, value = mean_proportion, poor_mean:excellent_mean) %>%
  mutate(rating = 
      factor(rating, c("poor_mean", "fair_mean", "good_mean", "very_good_mean", "excellent_mean"))) %>% 
  ggplot(aes(x = year, y = mean_proportion)) +
    geom_line(aes(color = state, alpha = 0.5)) +
    geom_smooth(method = "loess", se = TRUE) + 
    facet_grid(~rating) + 
    labs(
      title = "Locations observed by state, 2002-2010",
      x = "Year",
      y = "Number of observations",
      caption = "Data from BRFSS SMART 2010"
    ) +
    viridis::scale_color_viridis(
      name = "State", 
    discrete = TRUE
    ) +
    theme(legend.position = "none", 
          axis.text.x = element_text(size = 5))
```

<img src="p8105_hw3_sal2222_files/figure-markdown_github/BFRSS five panel plot-1.png" width="90%" />

The five-panel plot above displays the average proportion in each response category in the BRFSS "Overall Health" topic, averaged across county-level regions in each state, with a loess smoothed conditional mean curve and 95% confidence level. We see the distribution of state-level averages over time from 2002 to 2008. "Very good" makes up the highest proportion of responses, followed by "good", "excellent", "fair", and lastly "poor". For the most part, the trend of responses remained consistent over time. The proportion of "very good" responses increases slightly, and "excellent" responses decrease slightly over this period.

Problem 2: Instacart
====================

First, I will load the Instacart data.

``` r
data("instacart")

instacart
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord~ reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # ... with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

This Instacart Online Grocery Shopping 2017 dataset comes from an anonymized dataset with over 3 million online grocery orders from more than 200,000 Instacart users. This version of the data contains 1384617 observations of products in the orders. There are 15 variables in this dataset that describe order, product, and customer identifiers, the order in which the item was added to a user's cart, whether the product had been previously ordered by the user, the order sequence from the user, the hour and day of the week the order was placed, day since prior order, as well as product name, aisle, and department information. Our data contains 131209 unique orders for 39123 unique products. The average number of items in an order was 10.5527593.

*How many aisles are there, and which aisles are the most items ordered from?*

``` r
instacart %>% 
  summarize(n_distinct = n_distinct(aisle_id))
```

    ## # A tibble: 1 x 1
    ##   n_distinct
    ##        <int>
    ## 1        134

``` r
instacart %>%
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

There are **134** aisles in the database. The most items are ordered from **Fresh Vegetables (150,609), Fresh Fruits (150,773), and Package Vegetables and Fruits (78,493)**. Users seem to order healthy foods from this service.

*Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.*

``` r
instacart %>%
 group_by(aisle_id) %>% 
  summarize(n = n()) %>%
  ggplot(aes(x = aisle_id, y = n)) +
    geom_bar(stat = "identity") +
        labs(
     title = "Number of items ordered in each Instacart aisle",
      x = "Aisle ID",
      y = "Number of items ordered",
      caption = "Data from Instacart"
    )  +
    scale_x_continuous(breaks = c(0, 20, 40, 60, 80, 100, 120, 140)) +
    scale_y_continuous(labels = scales::comma)
```

<img src="p8105_hw3_sal2222_files/figure-markdown_github/plot items in aisles ordered by aisle ID-1.png" width="90%" />

The plot above shows the number of items ordered in each aisle with the x-axis arranged sequentially by the aisle identification number. This display may be useful to get an idea of layout efficiency. We see that popular aisles are fairly evenly dispersed throughout the span of aisles.

``` r
instacart %>%
 group_by(aisle_id) %>% 
  summarize(n = n()) %>%
  ggplot(aes(x = reorder(aisle_id, -n), y = n)) +
    geom_bar(stat = "identity") +
    labs(
     title = "Number of items ordered in each Instacart aisle",
      x = "Aisle ID",
      y = "Number of items ordered",
      caption = "Data from Instacart"
    ) +
    theme(axis.text.x = element_text(angle = 90, size = 4)) +
     scale_y_continuous(labels = scales::comma)
```

<img src="p8105_hw3_sal2222_files/figure-markdown_github/plot items in aisles ordered by number of items-1.png" width="90%" />

The plot above shows the number of items ordered in each aisle with the x-axis arranged from highest to lowest. This display is useful to visualize differences between the items ordered per aisle. However, because the 134 aisles are out of order, it is not easy to locate a specific aisle of interest.

*Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”*

``` r
instacart %>%
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  summarize(most_popular_item = first(product_name)) %>% 
  knitr::kable()
```

| aisle                      | most\_popular\_item |
|:---------------------------|:--------------------|
| baking ingredients         | Organic Corn Starch |
| dog food care              | Small Dog Biscuits  |
| packaged vegetables fruits | Super Greens Salad  |

The table above shows the most popular item in three selected aisles. The items (corn starch, dog biscuits, salad) make sense for their respective aisles.

*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*

``` r
instacart %>%
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  mutate(order_dow = recode_factor(order_dow,
       "0" = "Sunday",
       "1" = "Monday",
       "2" = "Tuesday",
       "3" = "Wednesday",
       "4" = "Thursday",
       "5" = "Friday",
       "6" = "Saturday")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_day) %>% 
  knitr::kable(digits = 2)
```

| product\_name    |  Sunday|  Monday|  Tuesday|  Wednesday|  Thursday|  Friday|  Saturday|
|:-----------------|-------:|-------:|--------:|----------:|---------:|-------:|---------:|
| Coffee Ice Cream |   13.22|   15.00|    15.33|      15.40|     15.17|   10.33|     12.35|
| Pink Lady Apples |   12.25|   11.68|    12.00|      13.94|     11.91|   13.87|     11.56|

The table above displays the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream were ordered on each day of the week. The mean hour values are based on a 24 hour clock, and represent fraction of an hour, rather than hours and minutes. From the `order_dow` variable, day `0` was assumed to represent Sunday. We see that every day except for Friday, the ice cream is ordered later in the day than the apples. Lunchtime and late afternoon are the most popular order times, except for the Friday morning coffee ice cream.

Problem 3: NY NOAA
==================

I will load the NY NOAA data from the p8105.datasets package.

``` r
data("ny_noaa")
ny_noaa
```

    ## # A tibble: 2,595,176 x 7
    ##    id          date        prcp  snow  snwd tmax  tmin 
    ##    <chr>       <date>     <int> <int> <int> <chr> <chr>
    ##  1 US1NYAB0001 2007-11-01    NA    NA    NA <NA>  <NA> 
    ##  2 US1NYAB0001 2007-11-02    NA    NA    NA <NA>  <NA> 
    ##  3 US1NYAB0001 2007-11-03    NA    NA    NA <NA>  <NA> 
    ##  4 US1NYAB0001 2007-11-04    NA    NA    NA <NA>  <NA> 
    ##  5 US1NYAB0001 2007-11-05    NA    NA    NA <NA>  <NA> 
    ##  6 US1NYAB0001 2007-11-06    NA    NA    NA <NA>  <NA> 
    ##  7 US1NYAB0001 2007-11-07    NA    NA    NA <NA>  <NA> 
    ##  8 US1NYAB0001 2007-11-08    NA    NA    NA <NA>  <NA> 
    ##  9 US1NYAB0001 2007-11-09    NA    NA    NA <NA>  <NA> 
    ## 10 US1NYAB0001 2007-11-10    NA    NA    NA <NA>  <NA> 
    ## # ... with 2,595,166 more rows

This dataset contains measurements from all New York state weather stations in the Global Historical Climatology Network (GHCN) from January 1, 1981 through December 31, 2010. It includes variables for maximum and minimum daily temperatures, along with daily precipitation, snowfall, and snow depth. The five measurement variables are grouped by station identification number and date, for a total of 7 variables. There are a total of 2595176 daily observations in this dataset. The units of snowfall and snow depth are in millimeters. Precipitation is in tenths of millimeters, and temperatures are in tenths of degrees Celsius.

Missing data, depending on the variable of interest, can be extensive. Approximately half of the stations only report precipitation. There are 1222433 complete observations with all five measurements. There are 145838 missing precipitation measurements, 381221 missing snowfall measurements, 591786 missing snow depth measurements, 1134358 maximum temperature measurements, and 1134420 minimum temperature measurements.

I will now clean the data and create separate variables for year, month, and day. I will also convert units of precipitation to millimeters and temperatures to degrees Celsius. To execute the temperature unit conversions, it was necessary to change the class from character to numeric.

``` r
ny_noaa <- ny_noaa %>%
  separate(., "date", c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.numeric(tmax),
            tmin = as.numeric(tmin),
            prcp = prcp / 10, 
            tmax = tmax / 10,
            tmin = tmin / 10)

ny_noaa
```

    ## # A tibble: 2,595,176 x 9
    ##    id          year  month day    prcp  snow  snwd  tmax  tmin
    ##    <chr>       <chr> <chr> <chr> <dbl> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001 2007  11    01       NA    NA    NA    NA    NA
    ##  2 US1NYAB0001 2007  11    02       NA    NA    NA    NA    NA
    ##  3 US1NYAB0001 2007  11    03       NA    NA    NA    NA    NA
    ##  4 US1NYAB0001 2007  11    04       NA    NA    NA    NA    NA
    ##  5 US1NYAB0001 2007  11    05       NA    NA    NA    NA    NA
    ##  6 US1NYAB0001 2007  11    06       NA    NA    NA    NA    NA
    ##  7 US1NYAB0001 2007  11    07       NA    NA    NA    NA    NA
    ##  8 US1NYAB0001 2007  11    08       NA    NA    NA    NA    NA
    ##  9 US1NYAB0001 2007  11    09       NA    NA    NA    NA    NA
    ## 10 US1NYAB0001 2007  11    10       NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

*For snowfall, what are the most commonly observed values? Why?*

``` r
ny_noaa %>% 
   group_by(snow) %>% 
   summarize(n = n()) %>% 
   top_n(10) %>% 
   arrange(desc(n))
```

    ## Selecting by n

    ## # A tibble: 10 x 2
    ##     snow       n
    ##    <int>   <int>
    ##  1     0 2008508
    ##  2    NA  381221
    ##  3    25   31022
    ##  4    13   23095
    ##  5    51   18274
    ##  6    76   10173
    ##  7     8    9962
    ##  8     5    9748
    ##  9    38    9197
    ## 10     3    8790

The most commonly observed value is **0 mm**, with over 2 million observations. This makes sense in New York State due to it seasonal weather. Also, even if there is light snow, the snowfall may not be measurable. Following observations where snowfall was not reported, the next most commonly observed values were **25 mm, 13 mm, and 51 mm**. As the snowfall was likely measured in inches, these common values correspond to 1 inch, half inch, and 2 inch reports.

*Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?*

*Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.*
